<div align="center">

<!-- You can swap this banner later (Canva / Figma / screenshots) -->
<img src="https://raw.githubusercontent.com/your-username/your-repo/main/assets/riskradar-banner.png" width="900" alt="RiskRadar banner"/>

# ğŸš¦ RiskRadar â€” IT Incident SLA Breach Risk (Decision Support)

**ML predicts SLAâ€‘breach risk. Dashboard shows the numbers + visuals. Optional LLM summary is *factâ€‘grounded* (no guessing).**

<br/>

![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python&logoColor=white)
![Streamlit](https://img.shields.io/badge/UI-Streamlit-ff4b4b?logo=streamlit&logoColor=white)
![ML](https://img.shields.io/badge/ML-scikit--learn-f7931e?logo=scikitlearn&logoColor=white)
![Data](https://img.shields.io/badge/Data-CSV-lightgrey?logo=files&logoColor=white)
![Status](https://img.shields.io/badge/Status-Completed-2ea44f)

</div>

---

## ğŸ” What this is

RiskRadar is a small **decision-support app** for IT incident teams.

You upload an incident event log (**incident_event_log.csv**) â†’ the pipeline builds **incident-level features** â†’ a trained model outputs an **SLA breach probability** for each incident â†’ the UI shows:

- âœ… **Overview**: total incidents, average risk, counts by risk band  
- ğŸ“‹ **Incident List**: searchable / sortable risk table  
- ğŸ§¾ **Incident Detail**: one incident, its computed features, risk band + recommended action  
- ğŸ“Š **Visuals**: distributions, top risky groups/categories, calibration plot, and a heatmap

Itâ€™s not a â€œperfect oracleâ€ project. Itâ€™s more like: *if youâ€™re triaging 25k tickets, where should you look first?*

---

## âœ… Why this is useful (in real life)

- âš¡ **Faster triage**: you can filter to High risk tickets quickly
- ğŸ§  **Consistency**: the risk score is based on the same feature rules every time
- ğŸ“ˆ **Ops insights**: you can see risky assignment groups / categories (patterns show up fast)
- ğŸ§¾ **Explainable enough**: explanations are built from computed facts (counts, durations, etc.)

---

## âœ¨ Features

- ğŸ“¤ Upload a CSV event log (ServiceNow-like incident event data)
- ğŸ§± Incident summary builder (event log â†’ incident level table)
- ğŸ¤– SLA breach risk prediction (probability + risk band: Low / Medium / High)
- ğŸ—‚ï¸ Sortable / filterable incident list + search by INC number
- ğŸ§¾ Incident detail view with:
  - computed features for that incident
  - risk + recommended action
  - optional LLM â€œshort explanationâ€ using only computed facts
- ğŸ“Š Dashboard visuals:
  - risk probability distribution
  - risk band counts
  - top risky assignment groups (avg probability)
  - top risky categories (avg probability)
  - calibration curve (how predicted probs match reality)
  - priority Ã— risk band heatmap (quick triage view)
- â¬‡ï¸ Download predictions as CSV

---

## ğŸ§© How the files connect (big picture)

Think of it like a simple pipeline:

1) **data_processing.py**  
   Reads the raw event log CSV and does basic cleanup (dates, missing values, column sanity).

2) **feature_engineering.py**  
   Turns event-level rows into **incident-level features** (counts, reassignments, reopen count, time-based stats, etc.).

3) **model_training.py** + **run_train.py**  
   Trains the model and saves it as `best_model.joblib`.  
   (You run training once. After that, you just load the model.)

4) **decision_logic.py**  
   Converts probability â†’ **risk band** + **recommended action** (simple rules, easy to explain).

5) **llm_explainer.py** (optional)  
   Generates a short explanation text. It only uses computed facts from the incident row.  
   If you donâ€™t add an API key, the app still works (it falls back to a non-LLM explanation).

6) **app.py**  
   Streamlit UI that ties everything together.

---

## ğŸ—‚ï¸ Project structure

```text
riskradar-it-incident-sla-risk/
â”œâ”€ app.py
â”œâ”€ data_processing.py
â”œâ”€ feature_engineering.py
â”œâ”€ model_training.py
â”œâ”€ run_train.py
â”œâ”€ decision_logic.py
â”œâ”€ llm_explainer.py
â”œâ”€ requirements.txt
â”œâ”€ incident_event_log.csv              # (optional) local copy (large)
â”œâ”€ best_model.joblib                   # trained model (generated after training)
â””â”€ RiskRadar_Report.ipynb              # your notebook report (optional)
```

---

## ğŸ› ï¸ Setup

### 1) Create a venv (recommended)

```bash
python -m venv .venv
```

**Windows (PowerShell):**
```bash
.venv\Scripts\Activate.ps1
```

**Mac/Linux:**
```bash
source .venv/bin/activate
```

### 2) Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸš€ How to run

### Option A: Run the app (normal)

```bash
streamlit run app.py
```

Open the URL shown in the terminal (usually `http://localhost:8501`).

### Option B: Train the model (only if you want to retrain)

```bash
python run_train.py
```

After training, it will create/update `best_model.joblib`.

---

## ğŸ¥ Demo video (add later)

Drop your demo video link here (GitHub supports video uploads in issues/PRs and gives you a link):

- **Demo video:** <YOUR_DEMO_VIDEO_LINK>

Tip: record a quick 60â€“90 sec walkthrough:
1) upload CSV  
2) show dashboard counts  
3) filter â€œHighâ€ risk  
4) open incident detail + explanation  
5) download predictions

---

## ğŸ“Š About the visuals (what each one means)

These small notes are here because otherwise people see graphs and go â€œokâ€¦but whyâ€ ğŸ˜…

- **Risk probability distribution**: shows how the model spreads predictions (lots of low, some high, etc.).  
- **Risk band counts**: quick count of Low/Medium/High based on your thresholds.  
- **Top risky assignment groups / categories**: average predicted risk by group/category (helps spot patterns).  
- **Calibration curve**: checks if predicted probabilities are realistic (e.g., â€œ0.8 means ~80% breachâ€).  
- **Priority Ã— Risk heatmap**: where risk is concentrated across priority labels (fast triage view).

---

## ğŸ§  Optional LLM explanations (how to enable)

By default, the app can show a **nonâ€‘LLM explanation** (template-based) so it works anywhere.

If you want the LLM version:
1) pick a provider (OpenAI / Azure OpenAI / etc.)
2) set an env var (example below)

**Example (OpenAI):**
```bash
setx OPENAI_API_KEY "your_key_here"
```

Then re-open your terminal and run:
```bash
streamlit run app.py
```

> Note: the LLM is only used to *summarise computed facts* (counts, durations, band, etc.).  
> It should not invent incident details that arenâ€™t in the CSV.

---

## ğŸ“Œ Dataset source & credits

This project uses the **Incident management process enriched event log** dataset from the UCI Machine Learning Repository.

**Citation (APA):**  
Amaral, C., Fantinato, M., & Peres, S. (2018). *Incident management process enriched event log* [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C57S4H

License: CC BY 4.0 (as listed on the dataset page).

---

## ğŸ§ª Notes / limitations (keeping it honest)

- This is a **prototype**. Real incident systems need access control, monitoring, audit logs, and careful evaluation.
- Labels/features depend on what the dataset provides. If a company tracks different fields, youâ€™d adapt feature engineering.
- If you retrain, results can change slightly (random splits / model settings).

---

## ğŸ™‹ Author

**Abinash Prasana Selvanathan**  
(Feel free to add LinkedIn/GitHub links here)

---

### â­ If you like it
If you found it useful, feel free to star the repo â€” it helps.
